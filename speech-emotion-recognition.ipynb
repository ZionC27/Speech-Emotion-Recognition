{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-31T20:58:51.955029Z","iopub.status.busy":"2024-03-31T20:58:51.954564Z","iopub.status.idle":"2024-03-31T20:59:08.476026Z","shell.execute_reply":"2024-03-31T20:59:08.474832Z","shell.execute_reply.started":"2024-03-31T20:58:51.954989Z"},"trusted":true},"outputs":[],"source":["#%pip install datasets\n","#%pip install pydub\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pylab as plt\n","import seaborn as sns\n","import tensorflow as tf\n","\n","import librosa\n","import librosa.display\n","import IPython.display as ipd\n","\n","from glob import glob\n","from IPython.display import Audio\n","from itertools import cycle\n","from tensorflow import keras\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","sns.set_theme(style=\"white\", palette=None)\n","color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n","color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n","plt.style.use('ggplot')\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TESS = '/TESS Toronto emotional speech set data/'\n","RAVDESS = '/RAVDESS Emotional speech audio/'\n","CREMA = '/CREMA-D/'\n","SAVEE = '/SAVEE/'\n","JL = '/JL corpus/'\n","EMOV = '/EMOV/'\n","ESD = '/ESD/'\n","ESD_F = '/ESD-F/'\n","ASVP_ESD = '/ASVP-ESD/'\n","DESD = '/DESD-E/'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tess_dir_list = os.listdir(TESS)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    'happy'   : 'happy',\n","    'neutral' : 'neutral',\n","    'sad'     : 'sad',\n","    'Sad'     : 'sad',\n","    'angry'   : 'angry',\n","    'fear'    : 'fear',\n","    'disgust'  : 'disgust',\n","}\n","\n","for directory in tess_dir_list:\n","    audio_files = os.listdir(os.path.join(TESS, directory))\n","    for audio_file in audio_files:\n","        part = audio_file.split('.')[0]\n","        key = part.split('_')[2]\n","        if key in emotion_dic:\n","            path_list.append(f\"{TESS}{directory}/{audio_file}\")\n","            gender_list.append('female') # female only dataset\n","            emotion_list.append(emotion_dic[key])\n","\n","tess_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","emotions_to_limit = ['angry', 'happy', 'neutral', 'sad']\n","\n","for emotion in emotions_to_limit:\n","    emotion_rows = tess_df[(tess_df['sex'] == 'female') & (tess_df['emotion'] == emotion)]\n","    \n","    # If there are more than 200 instances, remove some\n","    if len(emotion_rows) > 200:\n","        # Get the indices of the instances to remove\n","        indices_to_remove = emotion_rows.sample(200).index\n","        \n","        # Drop these instances\n","        tess_df = tess_df.drop(indices_to_remove)\n","\n","\n","\n","tess_df.head()\n","tess_df.emotion.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ravdess_dir_lis = os.listdir(RAVDESS)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    '03' : 'happy',\n","    '01' : 'neutral',\n","    '04' : 'sad',\n","    '05' : 'angry',\n","    '06' : 'fear',\n","    '07' : 'disgust',\n","}\n","\n","for directory in ravdess_dir_lis:\n","    actor_files = os.listdir(os.path.join(RAVDESS, directory))\n","    for audio_file in actor_files:\n","        part = audio_file.split('.')[0]\n","        key = part.split('-')[2]\n","        if key in emotion_dic:\n","            gender_code = int(part.split('-')[6])\n","            path_list.append(f\"{RAVDESS}{directory}/{audio_file}\")\n","            gender_list.append('female' if gender_code & 1 == 0 else 'male')\n","            emotion_list.append(emotion_dic[key])\n","\n","ravdess_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","ravdess_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["crema_dir_list = os.listdir(CREMA)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    'HAP' : 'happy',\n","    'NEU' : 'neutral',\n","    'SAD' : 'sad',\n","    'ANG' : 'angry',\n","    'FEA' : 'fear',\n","    'DIS' : 'disgust',\n","}\n","\n","female_id_list = [\n","    '1002', '1003', '1004', '1006', '1007', '1008', '1009', '1010', '1012', '1013', '1018',\n","    '1020', '1021', '1024', '1025', '1028', '1029', '1030', '1037', '1043', '1046', '1047',\n","    '1049', '1052', '1053', '1054', '1055', '1056', '1058', '1060', '1061', '1063', '1072',\n","    '1073', '1074', '1075', '1076', '1078', '1079', '1082', '1084', '1089', '1091',\n","]\n","\n","for audio_file in crema_dir_list:\n","    part = audio_file.split('_')\n","    key = part[2]\n","    if key in emotion_dic:\n","        path_list.append(f\"{CREMA}{audio_file}\")\n","        gender_list.append('female' if part[0] in female_id_list else 'male')\n","        emotion_list.append(emotion_dic[key])\n","\n","crema_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","crema_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["savee_dir_list = os.listdir(SAVEE)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    'h'  : 'happy',\n","    'n'  : 'neutral',\n","    'sa' : 'sad',\n","    'a'  : 'angry',\n","    'f'  : 'fear',\n","    'd'  : 'disgust'\n","}\n","\n","for audio_file in savee_dir_list:\n","    part = audio_file.split('_')[1]\n","    key = part[:-6]\n","    if key in emotion_dic:\n","        path_list.append(f\"{SAVEE}{audio_file}\")\n","        gender_list.append('male') # male only dataset\n","        emotion_list.append(emotion_dic[key])\n","\n","savee_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","savee_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["savee_dir_list = os.listdir(SAVEE)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    'h'  : 'happy',\n","    'n'  : 'neutral',\n","    'sa' : 'sad',\n","    'a'  : 'angry',\n","    'f'  : 'fear',\n","    'd'  : 'disgust'\n","}\n","\n","for audio_file in savee_dir_list:\n","    part = audio_file.split('_')[1]\n","    key = part[:-6]\n","    if key in emotion_dic:\n","        path_list.append(f\"{SAVEE}{audio_file}\")\n","        gender_list.append('male') # male only dataset\n","        emotion_list.append(emotion_dic[key])\n","\n","savee_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","savee_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["JL_dir_list = os.listdir(JL)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    'happy'   : 'happy',\n","    'sad'     : 'sad',\n","    'angry'   : 'angry',\n","    'neutral' : 'neutral',\n","    'excited' : 'happy',\n","}\n","\n","for audio_file in JL_dir_list:\n","    key = audio_file.split('_')[1]\n","    gender = audio_file.split('_')[0]\n","    if key in emotion_dic:\n","        path_list.append(f\"{JL}{audio_file}\")\n","        if gender[:-1] == 'female':\n","          gender_list.append('female')\n","        else:\n","          gender_list.append('male')\n","        emotion_list.append(emotion_dic[key])\n","\n","JL_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","JL_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["emov_dir_list = os.listdir(EMOV)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    'disgust'   : 'disgust',\n","    'anger'     : 'angry'\n","}\n","\n","for audio_file in emov_dir_list:\n","    key = audio_file.split('_')[1]\n","    gender = audio_file.split('_')[0]\n","    if key in emotion_dic:\n","        path_list.append(f\"{EMOV}{audio_file}\")\n","        if gender == 'female':\n","          gender_list.append('female')\n","        else:\n","          gender_list.append('male')\n","        emotion_list.append(emotion_dic[key])\n","\n","EMOV_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","EMOV_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ESD_dir_list = os.listdir(ESD)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    'happy'   : 'happy',\n","    'angry'   : 'angry',\n","    'anger'   : 'angry',\n","    'neutral' : 'neutral',\n","    'sad'     : 'sad'\n","}\n","\n","for audio_file in ESD_dir_list:\n","    key = audio_file.split('_')[1]\n","    gender = audio_file.split('_')[0]\n","    if key in emotion_dic:\n","        path_list.append(f\"{ESD}{audio_file}\")\n","        if gender == 'female':\n","          gender_list.append('female')\n","        else:\n","          gender_list.append('male')\n","        emotion_list.append(emotion_dic[key])\n","\n","ESD_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","ESD_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ESD_F_dir_list = os.listdir(ESD_F)\n","path_list = []\n","gender_list = []\n","emotion_list = []\n","\n","emotion_dic = {\n","    'happy'   : 'happy',\n","    'angry'   : 'angry',\n","    'anger'   : 'angry',\n","    'neutral' : 'neutral',\n","    'sad'     : 'sad'\n","}\n","\n","for audio_file in ESD_F_dir_list:\n","    key = audio_file.split('_')[1]\n","    gender = audio_file.split('_')[0]\n","    if key in emotion_dic:\n","        path_list.append(f\"{ESD_F}{audio_file}\")\n","        if gender == 'female':\n","          gender_list.append('female')\n","        else:\n","          gender_list.append('male')\n","        emotion_list.append(emotion_dic[key])\n","\n","ESD_F_df = pd.concat([\n","    pd.DataFrame(path_list, columns=['path']),\n","    pd.DataFrame(gender_list, columns=['sex']),\n","    pd.DataFrame(emotion_list, columns=['emotion'])\n","], axis=1)\n","\n","ESD_F_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine all datasets\n","\n","df = pd.concat([\n","    ravdess_df,\n","    tess_df,\n","    crema_df,\n","    savee_df,\n","    JL_df,\n","    EMOV_df,\n","    ESD_df,\n","    ESD_F_df\n","], axis=0)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocessing\n","\n","def plot_distribution(df):\n","    countTable = df.groupby(['emotion', 'sex']).count()\n","    pivotTable = countTable.pivot_table(index='emotion', columns='sex', values='path')\n","\n","    pivotTable.plot(kind='bar', figsize=(6, 6), color=['pink', 'blue'])\n","    plt.title('Emotion and Gender Distribution')\n","    plt.xlabel('Emotion')\n","    plt.ylabel('Count')\n","    plt.show()\n","\n","plot_distribution(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['sex'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.drop('sex', axis=1, inplace=True)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pydub import AudioSegment, effects\n","\n","emotion_dic = {\n","    'neutral' : 0,\n","    'happy'   : 1,\n","    'sad'     : 2,\n","    'angry'   : 3,\n","    'fear'    : 4,\n","    'disgust' : 5\n","}\n","\n","def encode(label):\n","    return emotion_dic.get(label)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import librosa\n","import random\n","from tqdm import tqdm\n","\n","# NOISE\n","def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n","    data = data + noise_amp * np.random.normal(size=data.shape[0])\n","    return data\n","\n","# STRETCH\n","def stretch(data, rate=0.9):\n","    return librosa.effects.time_stretch(data,rate=rate)\n","\n","# SHIFT\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-2, high=2) * 1000)\n","    return np.roll(data, shift_range)\n","\n","\n","\n","# Combined Augmentation Function\n","def augment_audio(data, sr):\n","    augmentations = [noise, stretch, shift]  # List of augmentation functions\n","    applied_augmentations = random.sample(augmentations, random.randint(1, 3))\n","\n","    for augmentation in applied_augmentations:\n","        data = augmentation(data)\n","\n","    return data\n","\n","# Example of using the augment_audio function\n","def preprocess_audio(path):\n","    raw_audio, sr = librosa.load(path,sr=16000)\n","    trimmed, _ = librosa.effects.trim(raw_audio, top_db=25, frame_length=256, hop_length=64)\n","    raw_audio = augment_audio(trimmed, sr)\n","    audio_duration=len(raw_audio)/sr\n","    if audio_duration > 4:\n","        raw_audio=raw_audio[:4*sr]\n","    else:\n","        raw_audio = np.pad(raw_audio, (0, (4*sr)-len(raw_audio)), 'constant')\n","\n","\n","    return raw_audio, sr\n","\n","\n","def preprocess_audio_aug(path):\n","    raw_audio, sr = librosa.load(path,sr=16000)\n","    trimmed, _ = librosa.effects.trim(raw_audio, top_db=25, frame_length=256, hop_length=64)\n","    raw_audio = augment_audio(trimmed, sr)\n","    audio_duration = len(raw_audio)/sr\n","    if audio_duration > 4:\n","        raw_audio = raw_audio[:4*sr]\n","    else:\n","        raw_audio = np.pad(raw_audio, (0, (4*sr)-len(raw_audio)), 'constant')\n","\n","\n","    return raw_audio, sr\n","\n","# normal\n","def preprocess_audio_n(path):\n","    raw_audio, sr = librosa.load(path,sr=16000)\n","    raw_audio, _ = librosa.effects.trim(raw_audio, top_db=25, frame_length=256, hop_length=64)\n","    audio_duration = len(raw_audio)/sr\n","    if audio_duration > 4:\n","        raw_audio = raw_audio[:4*sr]\n","    else:\n","        raw_audio = np.pad(raw_audio, (0, (4*sr)-len(raw_audio)), 'constant')\n","\n","\n","    return raw_audio, sr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["zcr_list = []\n","rms_list = []\n","mfccs_list = []\n","emotion_list = []\n","\n","FRAME_LENGTH = 400\n","HOP_LENGTH = 160\n","sr=16000\n","\n","# 20 mfcc features + 1 zcr + 1 rms\n","for row in tqdm(df.itertuples(index=False)):\n","    try:\n","        # normal preprocessing \n","        y,_= preprocess_audio_n(row.path)\n","\n","        zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","        rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=HOP_LENGTH)\n","\n","        zcr_list.append(zcr)\n","        rms_list.append(rms)\n","        mfccs_list.append(mfccs)\n","\n","        emotion_list.append(encode(row.emotion))\n","\n","        # augmentated preprocessing \n","        y,_= preprocess_audio_aug(row.path)\n","\n","        zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","        rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=HOP_LENGTH)\n","\n","        zcr_list.append(zcr)\n","        rms_list.append(rms)\n","        mfccs_list.append(mfccs)\n","\n","        emotion_list.append(encode(row.emotion))\n","\n","\n","        # oversample augmentated preprocessing for fear and disgust\n","        if row.emotion == \"fear\" and np.random.rand() <= 0.5 or row.emotion == \"disgust\" and np.random.rand() <= 0.3:\n","            y,_= preprocess_audio_aug(row.path)\n","\n","            zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","            rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=HOP_LENGTH)\n","\n","            zcr_list.append(zcr)\n","            rms_list.append(rms)\n","            mfccs_list.append(mfccs)\n","\n","            emotion_list.append(encode(row.emotion))\n","\n","        # y,_= preprocess_audio(row.path)\n","\n","        # zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","        # rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","        # mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=HOP_LENGTH)\n","\n","        # zcr_list.append(zcr)\n","        # rms_list.append(rms)\n","        # mfccs_list.append(mfccs)\n","\n","        # emotion_list.append(encode(row.emotion))\n","    except:\n","        print(f\"Failed for path: {row.path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Check shape\n","mfccs_list[7].shape"]},{"cell_type":"markdown","metadata":{},"source":["## Train test split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# combining the different features\n","X = np.concatenate((zcr_list,rms_list,mfccs_list),axis=1)\n","X = X.astype('float32')\n","\n","y = np.asarray(emotion_list)\n","y = np.expand_dims(y, axis=1).astype('int8')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save features\n","np.save('/X_Extract.npy', X)\n","np.save('/Y_Extract.npy', y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load features\n","X = np.load('/X_Extracted.npy')\n","y = np.load('/Y_Extracted.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras import layers, optimizers, callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_to_split, y_train, y_to_split = train_test_split(X, y, test_size=0.08, random_state=1)\n","X_val, X_test, y_val, y_test = train_test_split(X_to_split, y_to_split, test_size=0.3, random_state=1)\n","\n","y_val_class = to_categorical(y_val, 6)\n","y_test_class = to_categorical(y_test, 6)\n","\n","X_train.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Metric setup\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def summarize_history_accuracy(history):\n","\n","  plt.figure(figsize=(12, 6))\n","\n","  # Accuracy subplot\n","  plt.subplot(1, 2, 1)\n","  plt.plot(history.history['categorical_accuracy'])\n","  plt.plot(history.history['val_categorical_accuracy'])\n","  plt.title('Model Accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","  # Loss subplot\n","  plt.subplot(1, 2, 2)\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('Model Loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","  plt.tight_layout()\n","  plt.show()\n","\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def plot_confusion_matrix(xv, yv, MODEL):\n","    y_pred = np.argmax(MODEL.predict(xv), axis=1)\n","    labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']\n","    cm = confusion_matrix(np.argmax(yv, axis=1), y_pred, labels=range(6))\n","\n","    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n","    plt.xlabel('Predicted label')\n","    plt.ylabel('True label')\n","    plt.title('Confusion matrix')\n","    plt.show()\n","\n","    from tensorflow.keras.metrics import Precision, Recall, F1Score\n","\n","\n","def model_info(model, test_x, test_y):\n","    # Evaluate the model on validation data and get predictions\n","    loss, accuracy = model.evaluate(test_x, test_y)\n","    print(\"Validation Loss:\", loss)\n","    print(\"Validation Accuracy:\", accuracy)\n","\n","    # Create metric objects\n","    precision = Precision(name='precision')\n","    recall = Recall(name='recall')\n","    f1_score = F1Score(name='f1_score')\n","\n","    # Get model predictions\n","    y_pred = model.predict(test_x)\n","\n","    # Update metrics with true labels and predictions\n","    precision.update_state(test_y, y_pred)\n","    recall.update_state(test_y, y_pred)\n","    f1_score.update_state(test_y, y_pred)\n","\n","    # Print calculated metrics\n","    print(\"Precision:\", precision.result().numpy())\n","    print(\"Recall:\", recall.result().numpy())\n","\n","    # Calculate and print F1-score for each class\n","    f1_scores = f1_score.result().numpy()\n","    emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']  # Replace with your actual labels\n","\n","    for i, emotion in enumerate(emotion_labels):\n","        print(f\"F1-Score ({emotion}):\", f1_scores[i])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Callbacks\n","# Reduce learning rate when validation categorical accuracy does not improve for 'patience' epochs\n","# The learning rate will be reduced by a 'factor' of 0.5 (50% reduction)\n","rlrop = callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n","\n","# Early stopping to halt training when the validation loss doesn't improve for 'patience' epochs\n","early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Audio test setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["path = '/male1_angry_1.wav'\n","\n","\n","def preprocess_audio(path):\n","    raw_audio, sr = librosa.load(path,sr=16000)\n","    raw_audio, _ = librosa.effects.trim(raw_audio, top_db=25, frame_length=256, hop_length=64)\n","    audio_duration=len(raw_audio)/sr\n","    if audio_duration > 4:\n","        raw_audio=raw_audio[:4*sr]\n","    else:\n","        raw_audio = np.pad(raw_audio, (0, (4*sr)-len(raw_audio)), 'constant')\n","\n","\n","    return raw_audio, sr\n","\n","\n","\n","zcr_list = []\n","rms_list = []\n","mfccs_list = []\n","emotion_list = []\n","\n","FRAME_LENGTH = 400\n","HOP_LENGTH = 160\n","sr=16000\n","\n","\n","y,_= preprocess_audio(path)\n","zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n","mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=25, hop_length=HOP_LENGTH)\n","\n","zcr_list.append(zcr)\n","rms_list.append(rms)\n","mfccs_list.append(mfccs)\n","pda = np.concatenate((zcr_list,rms_list,mfccs_list),axis=1)\n","pda = pda.astype('float32')"]},{"cell_type":"markdown","metadata":{},"source":["# Model training "]},{"cell_type":"markdown","metadata":{},"source":["## LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the model\n","MODEL = Sequential()\n","MODEL.add(layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1:])))\n","MODEL.add(layers.Dropout(0.5))  # Adding dropout for regularization\n","MODEL.add(layers.LSTM(64))\n","MODEL.add(layers.Dropout(0.5))  # Adding dropout for regularization\n","MODEL.add(layers.Dense(6, activation='softmax'))\n","\n","# Compile the model with Adam optimizer\n","MODEL.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['categorical_accuracy'])\n","\n","print(MODEL.summary())\n","\n","# Fit the model with the callbacks\n","history = MODEL.fit(X_train, y_train_class,\n","                    epochs=200,\n","                    batch_size=20,  # Increased batch size for smoother gradients\n","                    validation_data=(X_val, y_val_class),\n","                    callbacks=[rlrop, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summarize_history_accuracy(history)\n","plot_confusion_matrix(X_test, y_test_class, MODEL)\n","model_info(MODEL, X_test, y_test_class)\n","MODEL.save('/model_LSTM_1_.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pr = MODEL.predict(pda)\n","print(pr)\n","emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']\n","predicted_emotion = emotion_labels[np.argmax(pr)]\n","print(predicted_emotion)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout\n","\n","model_lstm = Sequential()\n","model_lstm.add(layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1:])))\n","model_lstm.add(layers.Dropout(0.5))  # Adding dropout for regularization\n","model_lstm.add(layers.LSTM(64, return_sequences=True))\n","model_lstm.add(layers.Dropout(0.5))  # Adding dropout for regularization\n","model_lstm.add(layers.LSTM(64, return_sequences=True))\n","model_lstm.add(layers.Dropout(0.5))  # Adding dropout for regularization\n","model_lstm.add(layers.LSTM(64))\n","model_lstm.add(layers.Dropout(0.5))  # Adding dropout for regularization\n","\n","model_lstm.add(layers.Dense(128, activation='relu'))\n","model_lstm.add(layers.Dropout(0.5))  # Adding dropout for regularization\n","model_lstm.add(layers.Dense(64, activation='relu'))\n","model_lstm.add(layers.Dropout(0.5))  # Adding dropout for regularization\n","model_lstm.add(layers.Dense(6, activation='softmax'))\n","\n","opt = optimizers.Adam(clipvalue=0.5)  # clipvalue to prevent exploding gradients\n","model_lstm.compile(optimizer=opt,\n","              loss='categorical_crossentropy',\n","              metrics=['categorical_accuracy'])\n","print(model_lstm.summary())\n","history = model_lstm.fit(X_train, y_train_class,epochs=200, validation_data=(X_val, y_val_class), batch_size=64, callbacks=[rlrop, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summarize_history_accuracy(history)\n","plot_confusion_matrix(X_test, y_test_class, model_lstm)\n","model_info(model_lstm, X_test, y_test_class)\n","MODEL.save('/model_LSTM_2_.h5')\n","\n","pr = model_lstm.predict(pda)\n","print(pr)\n","emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']\n","predicted_emotion = emotion_labels[np.argmax(pr)]\n","print(predicted_emotion)"]},{"cell_type":"markdown","metadata":{},"source":["## GRU"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras.layers import GRU, Dropout, Dense\n","\n","model_GRU = keras.Sequential([\n","  keras.layers.Input(shape=X_train.shape[1:]),\n","  keras.layers.GRU(64, return_sequences=True, dropout=0.5),  # First GRU layer\n","  keras.layers.BatchNormalization(),  # Normalize activations before next layer\n","\n","  keras.layers.GRU(32, return_sequences=True, dropout=0.5),  # Second GRU with fewer units\n","\n","  # Additional GRU layers (optional)\n","  keras.layers.GRU(16, return_sequences=True, dropout=0.3),  # Third GRU with even fewer units\n","  keras.layers.GRU(8, dropout=0.3),  # Fourth GRU layer\n","\n","  # Additional normal layers (optional)\n","  keras.layers.Dense(64, activation='relu'),  # Dense layer with ReLU activation\n","  keras.layers.Dropout(0.2),  # Dropout for regularization\n","\n","  keras.layers.Dense(6, activation=\"softmax\")  # Output layer\n","])\n","\n","model_GRU.compile(optimizer='adam',\n","            loss='categorical_crossentropy',\n","            metrics=['categorical_accuracy'])\n","\n","print(model_GRU.summary())\n","\n","# Fit the model with the callbacks\n","history_GRU = model_GRU.fit(X_train, y_train_class,\n","                    epochs=200,\n","                    batch_size=20,  # Increased batch size for smoother gradients\n","                    validation_data=(X_val, y_val_class),\n","                    callbacks=[rlrop, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summarize_history_accuracy(history_GRU)\n","plot_confusion_matrix(X_test, y_test_class, model_GRU )\n","model_info(model_GRU, X_test, y_test_class)\n","\n","pr = model_GRU.predict(pda)\n","print(pr)\n","emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']\n","predicted_emotion = emotion_labels[np.argmax(pr)]\n","print(predicted_emotion)"]},{"cell_type":"markdown","metadata":{},"source":["## Bi-LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_lstm_bi = Sequential([\n","  layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(X_train.shape[1:])),\n","  layers.Dropout(0.5),\n","  layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n","  layers.Dropout(0.5),\n","  layers.Bidirectional(layers.LSTM(16)),\n","  layers.Dropout(0.5),\n","  layers.Dense(128, activation='relu'),  # Dense layer with ReLU activation\n","  layers.Dropout(0.2),\n","  layers.Dense(6, activation='softmax')  # Output layer\n","])\n","\n","\n","# Compile the model with Adam optimizer and gradient clipping\n","opt = optimizers.Adam(clipvalue=0.5)  # clipvalue to prevent exploding gradients\n","model_lstm_bi.compile(optimizer=opt,\n","              loss='categorical_crossentropy',\n","              metrics=['categorical_accuracy'])\n","\n","\n","print(model_lstm_bi.summary())\n","\n","# Fit the model with the callbacks\n","history1 = model_lstm_bi.fit(X_train, y_train_class,\n","                    epochs=200,\n","                    batch_size=32,  # Adjusted batch size\n","                    validation_data=(X_val, y_val_class),\n","                    callbacks=[rlrop, early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summarize_history_accuracy(history1)\n","plot_confusion_matrix(X_test, y_test_class, model_lstm_bi)\n","model_info(model_lstm_bi, X_test, y_test_class)\n","\n","pr = model_lstm_bi.predict(pda)\n","print(pr)\n","emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']\n","predicted_emotion = emotion_labels[np.argmax(pr)]\n","print(predicted_emotion)"]},{"cell_type":"markdown","metadata":{},"source":["## CNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow.keras.layers as L\n","\n","model_CNN = Sequential([\n","    L.Conv1D(512,kernel_size=5, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1:])),\n","    L.BatchNormalization(),\n","    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n","\n","    L.Conv1D(512,kernel_size=5,strides=1,padding='same',activation='relu'),\n","    L.BatchNormalization(),\n","    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n","    L.Dropout(0.2),  # Add dropout layer after the second max pooling layer\n","\n","    L.Conv1D(256,kernel_size=5,strides=1,padding='same',activation='relu'),\n","    L.BatchNormalization(),\n","    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n","\n","    L.Conv1D(256,kernel_size=3,strides=1,padding='same',activation='relu'),\n","    L.BatchNormalization(),\n","    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n","    L.Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n","\n","    L.Conv1D(128,kernel_size=3,strides=1,padding='same',activation='relu'),\n","    L.BatchNormalization(),\n","    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n","    L.Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n","\n","    L.Flatten(),\n","    L.Dense(512,activation='relu'),\n","    L.BatchNormalization(),\n","    L.Dense(6,activation='softmax')\n","])\n","\n","opt = optimizers.Adam(clipvalue=0.5)  # clipvalue to prevent exploding gradients\n","model_CNN.compile(optimizer=opt,\n","              loss='categorical_crossentropy',\n","              metrics=['categorical_accuracy'])\n","\n","print(model_CNN.summary())\n","\n","history_CNN = model_CNN.fit(X_train, y_train_class,epochs=200, validation_data=(X_val, y_val_class), batch_size=64, callbacks=[rlrop, early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summarize_history_accuracy(history_CNN)\n","plot_confusion_matrix(X_test, y_test_class, model_CNN)\n","model_info(model_CNN,X_test, y_test_class)\n","\n","pr = model_CNN.predict(pda)\n","print(pr)\n","emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']\n","predicted_emotion = emotion_labels[np.argmax(pr)]\n","print(predicted_emotion)"]},{"cell_type":"markdown","metadata":{},"source":["## C-LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow.keras.layers as L\n","\n","modelCLSTM = Sequential([\n","    L.Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X_train.shape[1:])),\n","    L.MaxPooling1D(pool_size=2, strides = 2, padding = 'same'),\n","    L.BatchNormalization(),\n","    L.Dropout(0.3),\n","\n","    L.Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'),\n","    L.MaxPooling1D(pool_size=2, strides = 2, padding = 'same'),\n","    L.BatchNormalization(),\n","    L.Dropout(0.3),\n","\n","    L.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'),\n","    L.MaxPooling1D(pool_size=2, strides = 2, padding = 'same'),\n","    L.BatchNormalization(),\n","    L.Dropout(0.3),\n","\n","    L.LSTM(128, return_sequences=True),\n","    L.Dropout(0.3),\n","\n","    L.LSTM(128, return_sequences=True),\n","    L.Dropout(0.3),\n","    L.LSTM(128),\n","    L.Dropout(0.3),\n","\n","    L.Dense(128, activation='relu'),\n","    #L.Dropout(0.3),\n","\n","    L.Dense(64, activation='relu'),\n","    #L.Dropout(0.3),\n","\n","    L.Dense(32, activation='relu'),\n","    #L.Dropout(0.3),\n","\n","    L.Dense(6, activation='softmax')\n","])\n","\n","\n","opt = optimizers.Adam(clipvalue=0.5)  # clipvalue to prevent exploding gradients\n","modelCLSTM.compile(optimizer=opt,\n","              loss='categorical_crossentropy',\n","              metrics=['categorical_accuracy'])\n","\n","# Callbacks with adjusted ReduceLROnPlateau parameters for more aggressive reduction\n","# rlrop = callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n","# early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n","\n","print(modelCLSTM.summary())\n","\n","#history_CLSTM = modelCLSTM.fit(X_train, y_train_class,epochs=200, validation_data=(X_val, y_val_class), batch_size=64, callbacks=[rlrop, early_stopping])\n","\n","history_CLSTM = modelCLSTM.fit(X_train, y_train_class,epochs=200, validation_data=(X_to_split, y_to_splitt), batch_size=64, callbacks=[rlrop, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summarize_history_accuracy(history_CLSTM)\n","plot_confusion_matrix(X_test, y_test_class, modelCLSTM )\n","model_info(modelCLSTM, X_test, y_test_class)\n","\n","pr = modelCLSTM.predict(pda)\n","print(pr)\n","emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']\n","predicted_emotion = emotion_labels[np.argmax(pr)]\n","print(predicted_emotion)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":107620,"sourceId":256618,"sourceType":"datasetVersion"},{"datasetId":1646928,"sourceId":3935528,"sourceType":"datasetVersion"},{"datasetId":63047,"sourceId":148037,"sourceType":"datasetVersion"},{"datasetId":325566,"sourceId":653195,"sourceType":"datasetVersion"},{"datasetId":338555,"sourceId":671851,"sourceType":"datasetVersion"},{"datasetId":316368,"sourceId":639622,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
